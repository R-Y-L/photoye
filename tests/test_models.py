#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Photoye æ¨¡å‹è¯Šæ–­è„šæœ¬
é€ä¸€æµ‹è¯•å„ä¸ªæ¨¡å‹çš„è¾“å…¥è¾“å‡ºï¼Œå¿«é€Ÿå®šä½é—®é¢˜

ä½¿ç”¨æ–¹å¼:
  python test_models.py <image_path> [model_profile]
  
ç¤ºä¾‹:
  python test_models.py "D:/Pictures/test.jpg" balanced
  python test_models.py "D:/Pictures/test.jpg" accuracy
"""

import sys
import os
import cv2
import numpy as np
from pathlib import Path

def test_face_detection(image_path: str):
    """æµ‹è¯•äººè„¸æ£€æµ‹æ¨¡å‹"""
    print("\n" + "="*60)
    print("æµ‹è¯• 1: äººè„¸æ£€æµ‹ (YuNet)")
    print("="*60)
    
    try:
        from models.opencv_yunet_detector import OpenCVYuNetDetector
        
        detector = OpenCVYuNetDetector()
        if detector.detector is None:
            print("âŒ YuNet æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„")
            return None
        
        print(f"âœ… YuNet æ¨¡å‹å·²åŠ è½½")
        print(f"ğŸ“· è¾“å…¥å›¾ç‰‡: {image_path}")
        
        # è¯»å–å›¾ç‰‡
        image = cv2.imread(image_path)
        if image is None:
            print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡: {image_path}")
            return None
        
        h, w = image.shape[:2]
        print(f"ğŸ“Š å›¾ç‰‡å°ºå¯¸: {w}x{h}")
        
        # æ£€æµ‹
        results = detector.detect(image_path)
        
        print(f"\nğŸ¯ æ£€æµ‹ç»“æœ: å‘ç° {len(results)} ä¸ªäººè„¸")
        for i, face in enumerate(results, 1):
            bbox = face.get('bbox', [])
            confidence = face.get('confidence', 0)
            landmarks = face.get('landmarks', [])
            print(f"  äººè„¸ {i}:")
            print(f"    - BBox: {bbox}")
            print(f"    - ç½®ä¿¡åº¦: {confidence:.4f}")
            if landmarks:
                print(f"    - å…³é”®ç‚¹æ•°: {len(landmarks)}")
        
        return results
        
    except Exception as e:
        print(f"âŒ äººè„¸æ£€æµ‹é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_face_recognition(image_path: str, faces: list):
    """æµ‹è¯•äººè„¸è¯†åˆ«æ¨¡å‹"""
    print("\n" + "="*60)
    print("æµ‹è¯• 2: äººè„¸è¯†åˆ« (Dlib / SFace)")
    print("="*60)
    
    if not faces:
        print("âš ï¸  æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œè·³è¿‡è¯†åˆ«æµ‹è¯•")
        return None
    
    try:
        from models.dlib_detector import DlibFaceRecognizer
        
        recognizer = DlibFaceRecognizer()
        if recognizer.recognizer is None:
            print("âŒ Dlib è¯†åˆ«æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„")
            return None
        
        print(f"âœ… Dlib è¯†åˆ«æ¨¡å‹å·²åŠ è½½")
        
        embeddings = []
        for i, face in enumerate(faces, 1):
            bbox = face.get('bbox', [])
            landmarks = face.get('landmarks')
            
            embedding = recognizer.get_embedding(image_path, bbox, landmarks)
            
            if embedding is not None:
                embeddings.append(embedding)
                print(f"\n  äººè„¸ {i} embedding:")
                print(f"    - ç»´åº¦: {embedding.shape}")
                print(f"    - èŒƒå›´: [{embedding.min():.4f}, {embedding.max():.4f}]")
                print(f"    - å‡å€¼: {embedding.mean():.4f}")
            else:
                print(f"\n  äººè„¸ {i}: æå–å¤±è´¥")
        
        return embeddings
        
    except Exception as e:
        print(f"âŒ äººè„¸è¯†åˆ«é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_scene_classification(image_path: str, model_profile: str = "balanced"):
    """æµ‹è¯•åœºæ™¯åˆ†ç±»æ¨¡å‹"""
    print("\n" + "="*60)
    print(f"æµ‹è¯• 3: åœºæ™¯åˆ†ç±» (MobileNetV2 | æ¨¡å‹æ¡£ä½: {model_profile})")
    print("="*60)
    
    try:
        from analyzer import AIAnalyzer
        
        analyzer = AIAnalyzer(model_profile=model_profile)
        
        print(f"âœ… åˆ†æå™¨å·²åˆå§‹åŒ–")
        print(f"   - æ£€æµ‹æ¨¡å‹: {analyzer.detector_type} -> {analyzer.face_detector.__class__.__name__}")
        print(f"   - è¯†åˆ«æ¨¡å‹: {analyzer.recognizer_type} -> {analyzer.face_recognizer.__class__.__name__}")
        print(f"   - åˆ†ç±»æ¨¡å‹: {analyzer.classifier_type} -> {analyzer.scene_classifier.__class__.__name__}")
        
        # æµ‹è¯•åˆ†ç±»å™¨
        if isinstance(analyzer.scene_classifier, str):
            print(f"âš ï¸  åˆ†ç±»å™¨æ˜¯å ä½ç¬¦: {analyzer.scene_classifier}")
            return None
        
        print(f"\nğŸ“· è¾“å…¥å›¾ç‰‡: {image_path}")
        
        # è¯»å–å›¾ç‰‡éªŒè¯
        image = cv2.imread(image_path)
        if image is None:
            print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡")
            return None
        
        # è°ƒç”¨åˆ†ç±»
        results = analyzer.classify_scene(image_path)
        
        print(f"\nğŸ¯ åˆ†ç±»ç»“æœ:")
        if results:
            sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
            for i, (category, score) in enumerate(sorted_results, 1):
                print(f"  {i}. {category}: {score:.4f}")
        else:
            print("  (ç©ºç»“æœ)")
        
        return results
        
    except Exception as e:
        print(f"âŒ åœºæ™¯åˆ†ç±»é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_openclip_classification(image_path: str):
    """æµ‹è¯• OpenCLIP é›¶æ ·æœ¬åˆ†ç±»"""
    print("\n" + "="*60)
    print("æµ‹è¯• 4: OpenCLIP é›¶æ ·æœ¬åˆ†ç±»")
    print("="*60)
    
    try:
        from models.openclip_zero_shot import OpenCLIPZeroShotClassifier
        
        classifier = OpenCLIPZeroShotClassifier()
        
        if not classifier.loaded:
            print("âŒ OpenCLIP æ¨¡å‹æœªåŠ è½½")
            return None
        
        print(f"âœ… OpenCLIP æ¨¡å‹å·²åŠ è½½")
        print(f"   - Vision æ¨¡å‹: {classifier.vision_model_path}")
        print(f"   - Text æ¨¡å‹: {classifier.text_model_path}")
        print(f"   - Tokenizer: {classifier.tokenizer_path}")
        
        # è¯»å–å›¾ç‰‡
        image = cv2.imread(image_path)
        if image is None:
            print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡")
            return None
        
        print(f"\nğŸ“· è¾“å…¥å›¾ç‰‡: {image_path}")
        h, w = image.shape[:2]
        print(f"ğŸ“Š å›¾ç‰‡å°ºå¯¸: {w}x{h}")
        
        # åˆ†ç±»
        results = classifier.classify(image_path)
        
        print(f"\nğŸ¯ åˆ†ç±»ç»“æœ:")
        if results:
            sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
            for i, (category, score) in enumerate(sorted_results, 1):
                print(f"  {i}. {category}: {score:.4f}")
        else:
            print("  (ç©ºç»“æœ)")
        
        return results
        
    except Exception as e:
        print(f"âŒ OpenCLIP åˆ†ç±»é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_full_pipeline(image_path: str, model_profile: str = "balanced"):
    """æµ‹è¯•å®Œæ•´åˆ†ææµç¨‹"""
    print("\n" + "="*60)
    print("æµ‹è¯• 5: å®Œæ•´åˆ†ææµç¨‹")
    print("="*60)
    
    try:
        from analyzer import AIAnalyzer
        
        analyzer = AIAnalyzer(model_profile=model_profile)
        
        result = analyzer.analyze_photo(image_path)
        
        if result:
            print(f"âœ… åˆ†æå®Œæˆ")
            print(f"\nç»“æœæ‘˜è¦:")
            print(f"  - æœ€ç»ˆåˆ†ç±»: {result.get('category', 'N/A')}")
            print(f"  - æ£€æµ‹äººè„¸æ•°: {len(result.get('faces', []))}")
            print(f"  - åœºæ™¯åˆ†ç±»: {result.get('scene_classification', {})}")
        else:
            print(f"âŒ åˆ†æå¤±è´¥")
        
        return result
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æµç¨‹é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹å¼: python test_models.py <image_path> [model_profile]")
        print("ç¤ºä¾‹: python test_models.py 'D:/Pictures/test.jpg' balanced")
        sys.exit(1)
    
    image_path = sys.argv[1]
    model_profile = sys.argv[2] if len(sys.argv) > 2 else "balanced"
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(image_path):
        print(f"âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        sys.exit(1)
    
    print(f"\nğŸ” Photoye æ¨¡å‹è¯Šæ–­å·¥å…·")
    print(f"æµ‹è¯•å›¾ç‰‡: {image_path}")
    print(f"æ¨¡å‹æ¡£ä½: {model_profile}")
    
    # é€ä¸ªæµ‹è¯•
    print("\n" + "#"*60)
    print("# ç¬¬ä¸€æ­¥: æµ‹è¯•äººè„¸æ£€æµ‹")
    print("#"*60)
    faces = test_face_detection(image_path)
    
    if faces:
        print("\n" + "#"*60)
        print("# ç¬¬äºŒæ­¥: æµ‹è¯•äººè„¸è¯†åˆ«")
        print("#"*60)
        embeddings = test_face_recognition(image_path, faces)
    
    print("\n" + "#"*60)
    print("# ç¬¬ä¸‰æ­¥: æµ‹è¯• MobileNetV2 åˆ†ç±»")
    print("#"*60)
    classification = test_scene_classification(image_path, model_profile)
    
    print("\n" + "#"*60)
    print("# ç¬¬å››æ­¥: æµ‹è¯• OpenCLIP é›¶æ ·æœ¬åˆ†ç±»")
    print("#"*60)
    openclip_result = test_openclip_classification(image_path)
    
    print("\n" + "#"*60)
    print("# ç¬¬äº”æ­¥: å®Œæ•´åˆ†ææµç¨‹")
    print("#"*60)
    full_result = test_full_pipeline(image_path, model_profile)
    
    print("\n" + "="*60)
    print("âœ… è¯Šæ–­å®Œæˆ")
    print("="*60)
    print("\nè¯Šæ–­å»ºè®®:")
    if not faces:
        print("  âš ï¸  æœªæ£€æµ‹åˆ°äººè„¸ -> æ£€æŸ¥ YuNet æ¨¡å‹æˆ–å›¾ç‰‡å†…å®¹")
    if not classification or all(v < 0.3 for v in classification.values()):
        print("  âš ï¸  åˆ†ç±»ç½®ä¿¡åº¦è¿‡ä½ -> æ£€æŸ¥ MobileNetV2 æ¨¡å‹æˆ–è¾“å…¥é¢„å¤„ç†")
    if not openclip_result or all(v < 0.3 for v in openclip_result.values()):
        print("  âš ï¸  OpenCLIP ç½®ä¿¡åº¦è¿‡ä½ -> æ£€æŸ¥ OpenCLIP æ¨¡å‹æˆ–æç¤ºè¯")
    print("\næ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ä¸Šæ–¹è¯¦ç»†è¾“å‡ºã€‚")


if __name__ == "__main__":
    main()
