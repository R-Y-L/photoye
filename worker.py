#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Photoye - åå°ä»»åŠ¡ç®¡ç†æ¨¡å—
è´Ÿè´£å¤„ç†æ‰€æœ‰è€—æ—¶æ“ä½œï¼Œé¿å…UIçº¿ç¨‹é˜»å¡

ç‰ˆæœ¬: 2.2 (è‡ªåŠ¨åŒ–æµæ°´çº¿)
"""

import os
import time
from pathlib import Path
from typing import List, Callable, Optional, Dict, Any
from PyQt6.QtCore import QThread, pyqtSignal, QObject
from PyQt6.QtGui import QImage, QPixmap
from database import (
    add_photo,
    add_photos_batch,
    is_photo_exist,
    update_photo_status,
    add_face_data,
    add_faces_batch,
    get_photo_status,
    get_photos_without_faces,
    update_photo_embedding_by_path,
    batch_update_photo_embeddings,
    get_photos_without_embedding,
    search_photos_by_embedding,
    get_all_face_embeddings_for_clustering,
    update_face_cluster_assignments,
    create_person_for_cluster,
    clear_all_ai_data,
)
from analyzer import AIAnalyzer


# ==================== ç¼©ç•¥å›¾å¼‚æ­¥åŠ è½½ ====================

class ThumbnailWorker(QThread):
    """
    ç¼©ç•¥å›¾ç”Ÿæˆå·¥ä½œçº¿ç¨‹
    
    åœ¨åå°ç”Ÿæˆç¼©ç•¥å›¾ï¼Œé¿å…UIå¡é¡¿
    """
    
    # ä¿¡å·ï¼š(æ–‡ä»¶è·¯å¾„, QPixmapç¼©ç•¥å›¾)
    thumbnail_ready = pyqtSignal(str, object)
    # ä¿¡å·ï¼šæ‰¹é‡å®Œæˆ
    batch_completed = pyqtSignal()
    
    def __init__(self, thumbnail_size: int = 150):
        super().__init__()
        self.thumbnail_size = thumbnail_size
        self.pending_paths: List[str] = []
        self.is_running = False
        self.should_stop = False
        self._lock = False  # ç®€å•é”
    
    def add_paths(self, paths: List[str]):
        """æ·»åŠ å¾…å¤„ç†çš„å›¾ç‰‡è·¯å¾„"""
        # å»é‡æ·»åŠ 
        existing = set(self.pending_paths)
        for p in paths:
            if p not in existing:
                self.pending_paths.append(p)
    
    def run(self):
        """çº¿ç¨‹ä¸»æ‰§è¡Œå‡½æ•°"""
        self.is_running = True
        self.should_stop = False
        
        while not self.should_stop:
            if not self.pending_paths:
                # æ²¡æœ‰å¾…å¤„ç†çš„ï¼Œä¼‘çœ ä¸€ä¸‹
                time.sleep(0.05)
                continue
            
            # å–å‡ºä¸€ä¸ªè·¯å¾„
            path = self.pending_paths.pop(0)
            
            try:
                pixmap = self._create_thumbnail(path)
                if pixmap:
                    self.thumbnail_ready.emit(path, pixmap)
            except Exception as e:
                print(f"ç”Ÿæˆç¼©ç•¥å›¾å¤±è´¥: {path}, é”™è¯¯: {e}")
            
            # å¦‚æœé˜Ÿåˆ—ç©ºäº†ï¼Œå‘é€æ‰¹é‡å®Œæˆä¿¡å·
            if not self.pending_paths:
                self.batch_completed.emit()
        
        self.is_running = False
    
    def _create_thumbnail(self, image_path: str) -> Optional[QPixmap]:
        """åˆ›å»ºç¼©ç•¥å›¾"""
        if not os.path.exists(image_path):
            return None
        
        image = QImage(image_path)
        if image.isNull():
            return None
        
        from PyQt6.QtCore import Qt
        thumbnail = image.scaled(
            self.thumbnail_size, self.thumbnail_size,
            Qt.AspectRatioMode.KeepAspectRatio,
            Qt.TransformationMode.FastTransformation  # ä½¿ç”¨å¿«é€Ÿå˜æ¢æå‡æ€§èƒ½
        )
        return QPixmap.fromImage(thumbnail)
    
    def stop(self):
        """åœæ­¢çº¿ç¨‹"""
        self.should_stop = True
        self.pending_paths.clear()


class ScanWorker(QThread):
    """
    æ™ºèƒ½æ‰«æå·¥ä½œçº¿ç¨‹ (V2.2 è‡ªåŠ¨åŒ–æµæ°´çº¿)
    
    å®Œæ•´æµç¨‹: æ‰«æ â†’ CLIPåˆ†ç±» â†’ äººè„¸æ£€æµ‹ â†’ äº¤å‰éªŒè¯ â†’ è‡ªåŠ¨èšç±»
    ç”¨æˆ·åªéœ€é€‰æ‹©æ–‡ä»¶å¤¹ï¼Œåå°è‡ªåŠ¨å®Œæˆæ‰€æœ‰AIå¤„ç†
    """
    
    # å®šä¹‰ä¿¡å·
    progress_updated = pyqtSignal(int, int)  # (current, total)
    stage_changed = pyqtSignal(str)  # å½“å‰é˜¶æ®µæè¿°
    file_found = pyqtSignal(str)  # filepath
    scan_completed = pyqtSignal(int)  # total_files
    pipeline_completed = pyqtSignal(dict)  # å®Œæ•´æµæ°´çº¿ç»“æœ
    error_occurred = pyqtSignal(str)  # error_message
    
    def __init__(self, root_path: str, supported_extensions: List[str] = None, model_profile: Optional[str] = None):
        """
        åˆå§‹åŒ–æ‰«æå·¥ä½œçº¿ç¨‹
        
        Args:
            root_path: è¦æ‰«æçš„æ ¹ç›®å½•è·¯å¾„
            supported_extensions: æ”¯æŒçš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨
            model_profile: æ¨¡å‹æ¡£ä½ï¼ˆç”¨äºAIåˆ†æï¼‰
        """
        super().__init__()
        
        self.root_path = root_path
        self.supported_extensions = supported_extensions or [
            '.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.webp'
        ]
        self.is_running = False
        self.should_stop = False
        self.model_profile = model_profile
        
        # ç»Ÿè®¡æ•°æ®
        self.stats = {
            'total_files': 0,
            'new_files': 0,
            'faces_detected': 0,
            'categories_corrected': 0,
            'clusters_created': 0,
            'noise_faces': 0
        }

        # åˆå§‹åŒ–AIç»„ä»¶
        self.clip_encoder = None
        self.scene_classifier = None
        self.ai_analyzer = None
        self._init_ai_components()
        
        print(f"æ‰«æå·¥ä½œçº¿ç¨‹åˆå§‹åŒ– (V2.2 è‡ªåŠ¨åŒ–æµæ°´çº¿)")
        print(f"æ ¹ç›®å½•: {root_path}")
        print(f"æ”¯æŒæ ¼å¼: {self.supported_extensions}")
    
    def _init_ai_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰AIç»„ä»¶"""
        # åˆå§‹åŒ– CLIP ç¼–ç å™¨ (ç”¨äºè¯­ä¹‰ embedding)
        try:
            from models.clip_embedding import CLIPEmbeddingEncoder
            self.clip_encoder = CLIPEmbeddingEncoder()
            if self.clip_encoder.is_available():
                print("âœ… CLIP Embedding ç¼–ç å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                print("âš ï¸ CLIP Embedding ç¼–ç å™¨ä¸å¯ç”¨")
                self.clip_encoder = None
        except Exception as e:
            print(f"âš ï¸ CLIP ç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.clip_encoder = None
        
        # åˆå§‹åŒ– OpenCLIP é›¶æ ·æœ¬åˆ†ç±»å™¨ (æ›¿ä»£ MobileNetV2ï¼Œæ›´å‡†ç¡®)
        try:
            from models.openclip_zero_shot import OpenCLIPZeroShotClassifier
            self.scene_classifier = OpenCLIPZeroShotClassifier()
            print("âœ… OpenCLIP é›¶æ ·æœ¬åˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ OpenCLIP é›¶æ ·æœ¬åˆ†ç±»å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            # å›é€€åˆ° MobileNetV2
            try:
                from models.mobilenetv2_classifier import MobileNetV2SceneClassifier
                self.scene_classifier = MobileNetV2SceneClassifier()
                print("âš ï¸ å›é€€åˆ° MobileNetV2 åˆ†ç±»å™¨")
            except Exception as e2:
                print(f"âš ï¸ åœºæ™¯åˆ†ç±»å™¨åˆå§‹åŒ–å¤±è´¥: {e2}")
                self.scene_classifier = None
        
        # åˆå§‹åŒ–AIåˆ†æå™¨ (ç”¨äºäººè„¸æ£€æµ‹å’Œè¯†åˆ«)
        try:
            self.ai_analyzer = AIAnalyzer(model_profile=self.model_profile)
            print("âœ… AIåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ (äººè„¸æ£€æµ‹+è¯†åˆ«)")
        except Exception as e:
            print(f"âš ï¸ AIåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.ai_analyzer = None
    
    def run(self):
        """çº¿ç¨‹ä¸»æ‰§è¡Œå‡½æ•° - å®Œæ•´çš„è‡ªåŠ¨åŒ–æµæ°´çº¿"""
        self.is_running = True
        self.should_stop = False
        self.stats = {k: 0 for k in self.stats}
        
        try:
            print(f"å¼€å§‹è‡ªåŠ¨åŒ–æµæ°´çº¿: {self.root_path}")
            
            if not os.path.exists(self.root_path):
                self.error_occurred.emit(f"ç›®å½•ä¸å­˜åœ¨: {self.root_path}")
                return
            
            # Stage 0: æ¸…ç©ºæ—§çš„ AI åˆ†ææ•°æ®
            self.stage_changed.emit("ğŸ—‘ï¸ æ¸…ç©ºæ—§æ•°æ®...")
            clear_all_ai_data()
            
            # Stage 1: æ‰«ææ–‡ä»¶
            self.stage_changed.emit("ğŸ“‚ æ‰«ææ–‡ä»¶...")
            image_files = self._scan_files()
            if self.should_stop:
                return
            
            # Stage 2: CLIP åˆ†ç±»ä¸ Embedding
            self.stage_changed.emit("ğŸ·ï¸ åœºæ™¯åˆ†ç±»ä¸­...")
            self._classify_and_embed(image_files)
            if self.should_stop:
                return
            
            # Stage 3: äººè„¸æ£€æµ‹ä¸ç‰¹å¾æå–
            self.stage_changed.emit("ğŸ‘¤ æ£€æµ‹äººè„¸...")
            self._detect_faces(image_files)
            if self.should_stop:
                return
            
            # Stage 4: è‡ªåŠ¨èšç±»
            self.stage_changed.emit("ğŸ”— äººè„¸èšç±»...")
            self._auto_clustering()
            if self.should_stop:
                return
            
            # å®Œæˆ
            self.stage_changed.emit("âœ… å¤„ç†å®Œæˆ")
            self.pipeline_completed.emit(self.stats)
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            self.error_occurred.emit(f"æµæ°´çº¿é”™è¯¯: {str(e)}")
        finally:
            self.is_running = False
    
    def _scan_files(self) -> List[str]:
        """Stage 1: æ‰«æç›®å½•æ”¶é›†å›¾ç‰‡æ–‡ä»¶"""
        image_files = []
        for root, dirs, files in os.walk(self.root_path):
            if self.should_stop:
                break
            for file in files:
                if any(file.lower().endswith(ext) for ext in self.supported_extensions):
                    full_path = os.path.join(root, file)
                    image_files.append(full_path)
        
        self.stats['total_files'] = len(image_files)
        print(f"å‘ç° {len(image_files)} ä¸ªå›¾ç‰‡æ–‡ä»¶")
        
        # æ‰¹é‡æ·»åŠ æ–°æ–‡ä»¶åˆ°æ•°æ®åº“
        new_files = [f for f in image_files if not is_photo_exist(f)]
        if new_files:
            add_photos_batch(new_files)
            self.stats['new_files'] = len(new_files)
            print(f"æ‰¹é‡æ·»åŠ  {len(new_files)} å¼ æ–°ç…§ç‰‡")
        
        self.scan_completed.emit(len(image_files))
        return image_files
    
    def _classify_and_embed(self, image_files: List[str]):
        """Stage 2: CLIP åˆ†ç±»ä¸ Embedding æå– (V2.3: Multi-Crop)"""
        total = len(image_files)
        
        for i, file_path in enumerate(image_files):
            if self.should_stop:
                break
            
            status_row = get_photo_status(file_path)
            if not status_row:
                continue
                
            photo_id, status, category = status_row
            
            # æå– CLIP embedding (V2.3: ä½¿ç”¨ Multi-Crop)
            if self.clip_encoder:
                try:
                    # ä¼˜å…ˆä½¿ç”¨ multi-cropï¼Œå›é€€åˆ°å•ä¸€è£å‰ª
                    if hasattr(self.clip_encoder, 'encode_image_multicrop'):
                        embedding = self.clip_encoder.encode_image_multicrop(file_path, n_crops=5)
                    else:
                        embedding = self.clip_encoder.encode_image(file_path)
                    
                    if embedding is not None:
                        update_photo_embedding_by_path(file_path, embedding)
                except Exception as e:
                    print(f"CLIP embedding æå–å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            
            # åœºæ™¯åˆ†ç±»ï¼ˆå¦‚æœå°šæœªåˆ†ç±»ï¼‰
            if not category and self.scene_classifier:
                try:
                    classification = self.scene_classifier.classify(file_path)
                    if classification:
                        best_category = max(classification.items(), key=lambda x: x[1])[0]
                        update_photo_status(photo_id, 'done', best_category)
                except Exception as e:
                    print(f"åˆ†ç±»å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            
            self.file_found.emit(file_path)
            self.progress_updated.emit(i + 1, total)
    
    def _detect_faces(self, image_files: List[str]):
        """Stage 3: äººè„¸æ£€æµ‹ä¸ç‰¹å¾æå– + äº¤å‰éªŒè¯"""
        if not self.ai_analyzer:
            print("âš ï¸ AIåˆ†æå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡äººè„¸æ£€æµ‹")
            return
        
        import json
        
        # è·å–æ‰€æœ‰ç…§ç‰‡è¿›è¡Œäººè„¸æ£€æµ‹ï¼ˆä¸å†é™åˆ¶åˆ†ç±»ï¼‰
        total = len(image_files)
        faces_batch = []
        
        for i, file_path in enumerate(image_files):
            if self.should_stop:
                break
            
            status_row = get_photo_status(file_path)
            if not status_row:
                continue
                
            photo_id, status, category = status_row
            
            try:
                # æ£€æµ‹äººè„¸
                faces = self.ai_analyzer.detect_faces(file_path)
                
                if faces:
                    # äº¤å‰éªŒè¯ï¼šå¦‚æœæ£€æµ‹åˆ°äººè„¸ï¼Œä½†åˆ†ç±»ä¸æ˜¯äººç‰©ç›¸å…³ï¼Œåˆ™ä¿®æ­£åˆ†ç±»
                    non_person_categories = ['é£æ™¯', 'å»ºç­‘', 'ç¾é£Ÿ', 'åŠ¨ç‰©', 'æ–‡æ¡£', 'å®¤å†…']
                    if category in non_person_categories:
                        # ä¿®æ­£åˆ†ç±»
                        new_category = 'åˆç…§' if len(faces) > 1 else 'å•äººç…§'
                        update_photo_status(photo_id, 'done', new_category)
                        self.stats['categories_corrected'] += 1
                        print(f"ğŸ“ äº¤å‰éªŒè¯ä¿®æ­£: {os.path.basename(file_path)} [{category}] â†’ [{new_category}]")
                    
                    # ä¸ºæ¯ä¸ªäººè„¸æå–ç‰¹å¾
                    for face in faces:
                        embedding = self.ai_analyzer.get_face_embedding(
                            file_path, 
                            face['bbox'],
                            face.get('landmarks')
                        )
                        if embedding is not None:
                            landmarks_json = None
                            if face.get('landmarks'):
                                landmarks_json = json.dumps(face['landmarks'])
                            
                            faces_batch.append({
                                'photo_id': photo_id,
                                'bbox': face['bbox'],
                                'embedding': embedding,
                                'confidence': face.get('confidence', 0.0),
                                'landmarks': landmarks_json
                            })
                            self.stats['faces_detected'] += 1
                    
                    # æ›´æ–°åˆ†ç±»ï¼ˆå¦‚æœå°šæœªåˆ†ç±»ï¼‰
                    if not category:
                        new_cat = 'å•äººç…§' if len(faces) == 1 else 'åˆç…§'
                        update_photo_status(photo_id, 'done', new_cat)
                
            except Exception as e:
                print(f"äººè„¸æ£€æµ‹å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            
            self.progress_updated.emit(i + 1, total)
            
            # æ¯50å¼ æ‰¹é‡æ’å…¥ä¸€æ¬¡
            if len(faces_batch) >= 50:
                add_faces_batch(faces_batch)
                faces_batch.clear()
        
        # æ’å…¥å‰©ä½™çš„äººè„¸æ•°æ®
        if faces_batch:
            add_faces_batch(faces_batch)
        
        print(f"äººè„¸æ£€æµ‹å®Œæˆ: {self.stats['faces_detected']} ä¸ªäººè„¸, {self.stats['categories_corrected']} ä¸ªåˆ†ç±»ä¿®æ­£")
    
    def _auto_clustering(self):
        """Stage 4: è‡ªåŠ¨èšç±»"""
        from clustering import cluster_faces_dbscan
        from database import create_person_for_single_face
        
        # è·å–æ‰€æœ‰æœªåˆ†é…çš„äººè„¸ embedding
        face_embeddings = get_all_face_embeddings_for_clustering()
        
        if not face_embeddings:
            print("æ²¡æœ‰éœ€è¦èšç±»çš„äººè„¸")
            return
        
        print(f"å¼€å§‹èšç±» {len(face_embeddings)} ä¸ªäººè„¸...")
        
        # æ‰§è¡Œ DBSCAN èšç±» (min_samples=2: è‡³å°‘2å¼ æ‰èƒ½å½¢æˆç°‡)
        result = cluster_faces_dbscan(
            face_embeddings,
            eps=0.6,  # è°ƒæ•´ï¼šæ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼Œé¿å…æŠŠä¸åŒäººèšåœ¨ä¸€èµ·
            min_samples=2
        )
        
        # ä¸ºæ¯ä¸ªèšç±»åˆ›å»ºäººç‰©å¹¶åˆ†é…äººè„¸
        assignments = {}  # face_id -> person_id
        
        for cluster_id, face_ids in result['clusters'].items():
            person_id = create_person_for_cluster(cluster_id)
            if person_id > 0:
                for face_id in face_ids:
                    assignments[face_id] = person_id
        
        # ä¸ºå™ªå£°ç‚¹ï¼ˆåªå‡ºç°ä¸€æ¬¡çš„äººè„¸ï¼‰åˆ›å»ºç‹¬ç«‹äººç‰©
        # è¿™æ ·ç¡®ä¿æ¯ä¸ªæ£€æµ‹åˆ°çš„äººè„¸éƒ½æœ‰å¯¹åº”çš„äººç‰©è®°å½•
        noise_persons_created = 0
        for face_id in result['noise_ids']:
            person_id = create_person_for_single_face(face_id)
            if person_id > 0:
                assignments[face_id] = person_id
                noise_persons_created += 1
        
        # æ›´æ–°æ•°æ®åº“
        update_face_cluster_assignments(assignments, [])  # ä¸å†æœ‰çœŸæ­£çš„å™ªå£°
        
        total_persons = result['n_clusters'] + noise_persons_created
        self.stats['clusters_created'] = total_persons
        self.stats['noise_faces'] = 0  # å™ªå£°å·²è½¬ä¸ºç‹¬ç«‹äººç‰©
        
        print(f"èšç±»å®Œæˆ: {result['n_clusters']} ä¸ªäººç‰©, {result['n_noise']} ä¸ªå™ªå£°")
    
    def stop_scan(self):
        """åœæ­¢æ‰«æ"""
        print("è¯·æ±‚åœæ­¢æ‰«æ")
        self.should_stop = True


class FaceAnalysisWorker(QThread):
    """
    äººè„¸åˆ†æå·¥ä½œçº¿ç¨‹
    
    ä¸“é—¨ç”¨äºäººè„¸æ£€æµ‹ä¸è¯†åˆ«ï¼Œç‹¬ç«‹äºç…§ç‰‡å¯¼å…¥æµç¨‹
    """
    
    # å®šä¹‰ä¿¡å·
    progress_updated = pyqtSignal(int, int)  # (current, total)
    face_detected = pyqtSignal(str, int)  # (filepath, face_count)
    analysis_completed = pyqtSignal(int, int)  # (total_photos, total_faces)
    error_occurred = pyqtSignal(str)  # error_message
    
    def __init__(self, library_path: str = None, model_profile: Optional[str] = None):
        """
        åˆå§‹åŒ–äººè„¸åˆ†æå·¥ä½œçº¿ç¨‹
        
        Args:
            library_path: é™åˆ¶åœ¨æŸä¸ªç›®å½•ä¸‹åˆ†æ
            model_profile: æ¨¡å‹æ¡£ä½
        """
        super().__init__()
        
        self.library_path = library_path
        self.model_profile = model_profile
        self.is_running = False
        self.should_stop = False
        
        # åˆå§‹åŒ–AIåˆ†æå™¨
        self.ai_analyzer = AIAnalyzer(model_profile=model_profile)
        
        print(f"äººè„¸åˆ†æå·¥ä½œçº¿ç¨‹åˆå§‹åŒ–")
        if library_path:
            print(f"åˆ†æç›®å½•: {library_path}")
    
    def run(self):
        """çº¿ç¨‹ä¸»æ‰§è¡Œå‡½æ•°"""
        self.is_running = True
        self.should_stop = False
        
        try:
            print("å¼€å§‹äººè„¸åˆ†æ...")
            self._analyze_faces()
        except Exception as e:
            import traceback
            traceback.print_exc()
            self.error_occurred.emit(f"äººè„¸åˆ†æé”™è¯¯: {str(e)}")
        finally:
            self.is_running = False
    
    def _analyze_faces(self):
        """åˆ†ææ‰€æœ‰éœ€è¦äººè„¸æ£€æµ‹çš„ç…§ç‰‡"""
        # è·å–éœ€è¦äººè„¸æ£€æµ‹çš„ç…§ç‰‡ï¼ˆåˆ†ç±»ä¸ºäººç‰©ç›¸å…³ä½†å°šæ— äººè„¸æ•°æ®ï¼‰
        photos = get_photos_without_faces(self.library_path)
        
        if not photos:
            print("æ²¡æœ‰éœ€è¦äººè„¸åˆ†æçš„ç…§ç‰‡")
            self.analysis_completed.emit(0, 0)
            return
        
        total_photos = len(photos)
        total_faces = 0
        processed = 0
        
        print(f"éœ€è¦åˆ†æ {total_photos} å¼ ç…§ç‰‡")
        
        # æ”¶é›†æ‰€æœ‰äººè„¸æ•°æ®ç”¨äºæ‰¹é‡æ’å…¥
        faces_batch = []
        
        for photo in photos:
            if self.should_stop:
                break
            
            photo_id = photo['id']
            file_path = photo['filepath']
            
            try:
                # æ£€æµ‹äººè„¸
                faces = self.ai_analyzer.detect_faces(file_path)
                
                if faces:
                    # ä¸ºæ¯ä¸ªäººè„¸æå–ç‰¹å¾
                    for face in faces:
                        embedding = self.ai_analyzer.get_face_embedding(
                            file_path, 
                            face['bbox'],
                            face.get('landmarks')
                        )
                        if embedding is not None:
                            # åºåˆ—åŒ– landmarks ä¸º JSON å­—ç¬¦ä¸²
                            import json
                            landmarks_json = None
                            if face.get('landmarks'):
                                landmarks_json = json.dumps(face['landmarks'])
                            
                            faces_batch.append({
                                'photo_id': photo_id,
                                'bbox': face['bbox'],
                                'embedding': embedding,
                                'confidence': face.get('confidence', 0.0),
                                'landmarks': landmarks_json
                            })
                            total_faces += 1
                    
                    self.face_detected.emit(file_path, len(faces))
                    
                    # æ ¹æ®äººè„¸æ•°é‡æ›´æ–°åˆ†ç±»
                    if len(faces) == 1:
                        update_photo_status(photo_id, 'done', 'å•äººç…§')
                    elif len(faces) > 1:
                        update_photo_status(photo_id, 'done', 'åˆç…§')
                
            except Exception as e:
                print(f"äººè„¸åˆ†æå¤±è´¥: {file_path}, é”™è¯¯: {e}")
            
            processed += 1
            self.progress_updated.emit(processed, total_photos)
            
            # æ¯50å¼ æ‰¹é‡æ’å…¥ä¸€æ¬¡
            if len(faces_batch) >= 50:
                add_faces_batch(faces_batch)
                faces_batch.clear()
        
        # æ’å…¥å‰©ä½™çš„äººè„¸æ•°æ®
        if faces_batch:
            add_faces_batch(faces_batch)
        
        if not self.should_stop:
            self.analysis_completed.emit(processed, total_faces)
            print(f"äººè„¸åˆ†æå®Œæˆ: {processed} å¼ ç…§ç‰‡, {total_faces} ä¸ªäººè„¸")
    
    def stop(self):
        """åœæ­¢åˆ†æ"""
        print("è¯·æ±‚åœæ­¢äººè„¸åˆ†æ")
        self.should_stop = True
    def stop_scan(self):
        """
        åœæ­¢æ‰«æ
        """
        print("è¯·æ±‚åœæ­¢æ‰«æ")
        self.should_stop = True


class AnalysisWorker(QThread):
    """
    AIåˆ†æå·¥ä½œçº¿ç¨‹
    
    åœ¨é˜¶æ®µ0ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªå ä½ç±»
    å®é™…çš„AIåˆ†æåŠŸèƒ½å°†åœ¨é˜¶æ®µ3ä¸­å®ç°
    """
    
    # å®šä¹‰ä¿¡å·
    progress_updated = pyqtSignal(int, int)  # (current, total)
    photo_analyzed = pyqtSignal(str, dict)  # (filepath, analysis_result)
    analysis_completed = pyqtSignal(int)  # total_analyzed
    error_occurred = pyqtSignal(str)  # error_message
    
    def __init__(self, photo_list: List[str]):
        """
        åˆå§‹åŒ–åˆ†æå·¥ä½œçº¿ç¨‹
        
        Args:
            photo_list: å¾…åˆ†æçš„ç…§ç‰‡è·¯å¾„åˆ—è¡¨
        """
        super().__init__()
        
        self.photo_list = photo_list
        self.is_running = False
        self.should_stop = False
        
        print(f"[å ä½] åˆ†æå·¥ä½œçº¿ç¨‹åˆå§‹åŒ–ï¼Œå¾…åˆ†æç…§ç‰‡: {len(photo_list)} å¼ ")
    
    def run(self):
        """
        çº¿ç¨‹ä¸»æ‰§è¡Œå‡½æ•°
        """
        self.is_running = True
        self.should_stop = False
        
        try:
            print(f"[å ä½] å¼€å§‹AIåˆ†æ")
            
            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼š:
            # 1. åˆ›å»ºAIåˆ†æå™¨å®ä¾‹
            # 2. é€ä¸€åˆ†ææ¯å¼ ç…§ç‰‡
            # 3. å°†åˆ†æç»“æœå­˜å…¥æ•°æ®åº“
            # 4. å‘é€è¿›åº¦æ›´æ–°ä¿¡å·
            
            # å ä½å®ç° - æ¨¡æ‹Ÿåˆ†æè¿‡ç¨‹
            self._simulate_analysis()
            
        except Exception as e:
            self.error_occurred.emit(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        finally:
            self.is_running = False
    
    def _simulate_analysis(self):
        """
        æ¨¡æ‹Ÿåˆ†æè¿‡ç¨‹ (å ä½å‡½æ•°)
        """
        total_photos = len(self.photo_list)
        analyzed_count = 0
        
        for i, photo_path in enumerate(self.photo_list):
            if self.should_stop:
                break
            
            # æ¨¡æ‹Ÿåˆ†ææ—¶é—´
            time.sleep(0.5)
            
            # æ¨¡æ‹Ÿåˆ†æç»“æœ
            mock_result = {
                'category': 'å•äººç…§' if i % 3 == 0 else ('åˆç…§' if i % 3 == 1 else 'é£æ™¯'),
                'faces_count': i % 3 if i % 3 != 2 else 0,
                'confidence': 0.85 + (i % 10) * 0.01
            }
            
            # å‘é€åˆ†æç»“æœä¿¡å·
            self.photo_analyzed.emit(photo_path, mock_result)
            
            analyzed_count += 1
            
            # å‘é€è¿›åº¦æ›´æ–°ä¿¡å·
            self.progress_updated.emit(analyzed_count, total_photos)
        
        # å‘é€å®Œæˆä¿¡å·
        if not self.should_stop:
            self.analysis_completed.emit(analyzed_count)
    
    def stop_analysis(self):
        """
        åœæ­¢åˆ†æ
        """
        print("[å ä½] è¯·æ±‚åœæ­¢åˆ†æ")
        self.should_stop = True


class ClusteringWorker(QThread):
    """
    äººè„¸èšç±»å·¥ä½œçº¿ç¨‹
    
    ä½¿ç”¨ DBSCAN ç®—æ³•å¯¹äººè„¸ç‰¹å¾è¿›è¡Œèšç±»ï¼Œ
    èƒ½æ›´å¥½åœ°å¤„ç†å™ªå£°ç‚¹ï¼ˆç¦»ç¾¤äººè„¸ï¼‰
    """
    
    # å®šä¹‰ä¿¡å·
    progress_updated = pyqtSignal(int, int)  # (current, total)
    clustering_completed = pyqtSignal(dict)  # clustering result
    error_occurred = pyqtSignal(str)  # error_message
    
    def __init__(self, eps: float = 0.7, min_samples: int = 2):
        """
        åˆå§‹åŒ–èšç±»å·¥ä½œçº¿ç¨‹
        
        Args:
            eps: DBSCAN é‚»åŸŸåŠå¾„ï¼ˆä½™å¼¦è·ç¦»ï¼‰ï¼Œæ¨è 0.5-0.8
            min_samples: å½¢æˆç°‡çš„æœ€å°æ ·æœ¬æ•°
        """
        super().__init__()
        
        self.eps = eps
        self.min_samples = min_samples
        self.is_running = False
        self.should_stop = False
        
        print(f"èšç±»å·¥ä½œçº¿ç¨‹åˆå§‹åŒ–: eps={eps}, min_samples={min_samples}")
    
    def run(self):
        """çº¿ç¨‹ä¸»æ‰§è¡Œå‡½æ•°"""
        self.is_running = True
        self.should_stop = False
        
        try:
            print("å¼€å§‹ DBSCAN äººè„¸èšç±»...")
            self._perform_clustering()
        except Exception as e:
            import traceback
            traceback.print_exc()
            self.error_occurred.emit(f"èšç±»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        finally:
            self.is_running = False
    
    def _perform_clustering(self):
        """æ‰§è¡Œ DBSCAN èšç±»"""
        from database import (
            get_all_face_embeddings_for_clustering,
            update_face_cluster_assignments,
            create_person_for_cluster
        )
        from clustering import cluster_faces_dbscan
        
        # è·å–æ‰€æœ‰æœªåˆ†é…çš„äººè„¸ embedding
        self.progress_updated.emit(0, 100)
        face_embeddings = get_all_face_embeddings_for_clustering()
        
        if not face_embeddings:
            print("æ²¡æœ‰éœ€è¦èšç±»çš„äººè„¸")
            self.clustering_completed.emit({
                'n_clusters': 0,
                'n_noise': 0,
                'n_faces': 0
            })
            return
        
        print(f"è·å–åˆ° {len(face_embeddings)} ä¸ªå¾…èšç±»äººè„¸")
        self.progress_updated.emit(20, 100)
        
        if self.should_stop:
            return
        
        # æ‰§è¡Œ DBSCAN èšç±»
        result = cluster_faces_dbscan(
            face_embeddings,
            eps=self.eps,
            min_samples=self.min_samples
        )
        
        self.progress_updated.emit(60, 100)
        
        if self.should_stop:
            return
        
        # ä¸ºæ¯ä¸ªèšç±»åˆ›å»ºäººç‰©å¹¶åˆ†é…äººè„¸
        assignments = {}  # face_id -> person_id
        
        for cluster_id, face_ids in result['clusters'].items():
            # åˆ›å»ºæ–°äººç‰©
            person_id = create_person_for_cluster(cluster_id)
            if person_id > 0:
                for face_id in face_ids:
                    assignments[face_id] = person_id
        
        self.progress_updated.emit(80, 100)
        
        if self.should_stop:
            return
        
        # æ›´æ–°æ•°æ®åº“
        update_face_cluster_assignments(assignments, result['noise_ids'])
        
        self.progress_updated.emit(100, 100)
        
        # å‘é€å®Œæˆä¿¡å·
        final_result = {
            'n_clusters': result['n_clusters'],
            'n_noise': result['n_noise'],
            'n_faces': len(face_embeddings),
            'clusters': result['clusters']
        }
        
        print(f"èšç±»å®Œæˆ: {result['n_clusters']} ä¸ªäººç‰©, {result['n_noise']} ä¸ªå™ªå£°")
        self.clustering_completed.emit(final_result)
    
    def stop_clustering(self):
        """åœæ­¢èšç±»"""
        print("è¯·æ±‚åœæ­¢èšç±»")
        self.should_stop = True


# ==================== è¯­ä¹‰æœç´¢ ====================

class SemanticSearchWorker(QThread):
    """
    è¯­ä¹‰æœç´¢å·¥ä½œçº¿ç¨‹ (V2.3: Prompt Ensemble)
    
    ä½¿ç”¨ CLIP æ–‡æœ¬ç¼–ç å™¨å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡ï¼Œ
    ç„¶åä¸æ•°æ®åº“ä¸­çš„å›¾ç‰‡å‘é‡è®¡ç®—ç›¸ä¼¼åº¦
    """
    
    # ä¿¡å·
    search_completed = pyqtSignal(list)  # List of (photo_id, filepath, similarity)
    error_occurred = pyqtSignal(str)
    
    def __init__(self, query: str, top_k: int = 20, threshold: float = 0.25, use_ensemble: bool = True):
        """
        åˆå§‹åŒ–è¯­ä¹‰æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            threshold: æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆV2.3 é™ä½é˜ˆå€¼ä»¥é€‚é… ensembleï¼‰
            use_ensemble: æ˜¯å¦ä½¿ç”¨ Prompt Ensemble (V2.3)
        """
        super().__init__()
        self.query = query
        self.top_k = top_k
        self.threshold = threshold
        self.use_ensemble = use_ensemble
        self.clip_encoder = None
    
    def run(self):
        """æ‰§è¡Œè¯­ä¹‰æœç´¢"""
        try:
            # åˆå§‹åŒ– CLIP ç¼–ç å™¨
            from models.clip_embedding import CLIPEmbeddingEncoder
            self.clip_encoder = CLIPEmbeddingEncoder()
            
            if not self.clip_encoder.is_available():
                self.error_occurred.emit("CLIP ç¼–ç å™¨ä¸å¯ç”¨")
                return
            
            # ç¼–ç æŸ¥è¯¢æ–‡æœ¬ (V2.3: ä½¿ç”¨ Prompt Ensemble)
            if self.use_ensemble and hasattr(self.clip_encoder, 'encode_text_ensemble'):
                query_embedding = self.clip_encoder.encode_text_ensemble(self.query)
            else:
                query_embedding = self.clip_encoder.encode_text(self.query)
                
            if query_embedding is None:
                self.error_occurred.emit("æ–‡æœ¬ç¼–ç å¤±è´¥")
                return
            
            # æœç´¢ç›¸ä¼¼ç…§ç‰‡
            results = search_photos_by_embedding(
                query_embedding,
                top_k=self.top_k,
                threshold=self.threshold
            )
            
            # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
            filtered_results = [
                r for r in results if r[2] >= self.threshold
            ]
            
            self.search_completed.emit(filtered_results)
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            self.error_occurred.emit(f"æœç´¢é”™è¯¯: {str(e)}")


def main():
    """
    ä¸»å‡½æ•° - ç”¨äºç‹¬ç«‹æµ‹è¯•åå°ä»»åŠ¡æ¨¡å—
    """
    print("=" * 50)
    print("Photoye åå°ä»»åŠ¡æ¨¡å—æµ‹è¯• (é˜¶æ®µ0)")
    print("=" * 50)
    
    from PyQt6.QtWidgets import QApplication
    import sys
    
    app = QApplication(sys.argv)
    
    # æµ‹è¯•æ‰«æå·¥ä½œçº¿ç¨‹
    print("\næµ‹è¯•æ–‡ä»¶æ‰«æå·¥ä½œçº¿ç¨‹...")
    scan_worker = ScanWorker("/test/path")
    
    def on_progress(current, total):
        print(f"æ‰«æè¿›åº¦: {current}/{total}")
    
    def on_file_found(filepath):
        print(f"å‘ç°æ–‡ä»¶: {filepath}")
    
    def on_scan_completed(total):
        print(f"æ‰«æå®Œæˆï¼Œå…±å‘ç° {total} ä¸ªæ–‡ä»¶")
        app.quit()
    
    def on_error(error):
        print(f"å‘ç”Ÿé”™è¯¯: {error}")
        app.quit()
    
    # è¿æ¥ä¿¡å·
    scan_worker.progress_updated.connect(on_progress)
    scan_worker.file_found.connect(on_file_found)
    scan_worker.scan_completed.connect(on_scan_completed)
    scan_worker.error_occurred.connect(on_error)
    
    # å¯åŠ¨çº¿ç¨‹
    scan_worker.start()
    
    print("\nåå°ä»»åŠ¡æ¨¡å—æµ‹è¯•å®Œæˆï¼")
    print("æ³¨æ„: å½“å‰ä¸ºå ä½å®ç°ï¼Œå®é™…åŠŸèƒ½å°†åœ¨åç»­é˜¶æ®µå¼€å‘")
    
    # è¿è¡Œäº‹ä»¶å¾ªç¯
    sys.exit(app.exec())


if __name__ == "__main__":
    main()